{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import zipfile\n",
        "import csv\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder"
      ],
      "metadata": {
        "id": "ITrL3pjrJRJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive to access the data and save output\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"drive/MyDrive/SI_4lab/2D/data\"\n",
        "output_dir = \"drive/MyDrive/SI_4lab/2D\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "np.random.seed(69)\n",
        "torch.manual_seed(69)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(69)\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5jnUZnSJS18",
        "outputId": "d14d1f02-bc17-46af-f208-eaa5d3ea7048"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Using device: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a dictionary of hyperparameters\n",
        "HYPERPARAMS = {\n",
        "    # Data parameters\n",
        "    'img_size': 224,       # Size to resize images to\n",
        "    'batch_size': 20,      # Number of images per batch\n",
        "    'test_size': 0.1,      # Proportion of data for the test set\n",
        "    'val_size': 0.1,       # Proportion of data for the validation set\n",
        "\n",
        "    # Training parameters\n",
        "    'epochs': 50,          # Maximum number of training epochs\n",
        "    'patience': 10,        # Number of epochs with no improvement before early stopping\n",
        "    'learning_rate': 0.001, # Learning rate for the optimizer\n",
        "\n",
        "    # Model parameters\n",
        "    'architecture': 'simple',  # 'simple', 'medium', or 'complex' CNN architecture\n",
        "    'dropout_rate': 0.5,   # Dropout rate for regularization\n",
        "    'use_batch_norm': False, # Whether to use batch normalization\n",
        "    'activation': 'relu',  # Activation function for hidden layers ('relu', 'tanh', 'elu', or 'selu')\n",
        "    'optimizer_name': 'adam',  # Optimizer to use ('adam', 'sgd', or 'rmsprop')\n",
        "    'kernel_size': 3,      # Kernel size for convolutional layers\n",
        "    'num_filters': 32,     # Base number of filters in the first convolutional layer\n",
        "    'pool_size': 2         # Size of the max pooling window\n",
        "}"
      ],
      "metadata": {
        "id": "uSNJklzMYgxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to define image transformations for training, validation, and testing\n",
        "def get_transforms(img_size=HYPERPARAMS['img_size']):\n",
        "    # Transformations for the training set (includes data augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),  # Resize images\n",
        "        transforms.RandomHorizontalFlip(),       # Randomly flip images horizontally\n",
        "        transforms.RandomRotation(10),          # Randomly rotate images by up to 10 degrees\n",
        "        transforms.ToTensor(),                  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Transformations for validation and test sets (no data augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),  # Resize images\n",
        "        transforms.ToTensor(),                  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform"
      ],
      "metadata": {
        "id": "Sk3N-F-dJavr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Custom PyTorch Dataset for loading images from a list of file paths\n",
        "class SplitImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths  # List of image file paths\n",
        "        self.labels = labels            # List of corresponding labels\n",
        "        self.transform = transform      # Image transformations to apply\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of images in the dataset\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image path and label for a given index\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\") # Open and convert image to RGB\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ],
      "metadata": {
        "id": "3uRtTqpxUobM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to prepare the data: load images, split into train/val/test sets, and create DataLoaders\n",
        "def prepare_data(data_dir, batch_size=HYPERPARAMS['batch_size'],\n",
        "                test_size=HYPERPARAMS['test_size'], val_size=HYPERPARAMS['val_size']):\n",
        "    # Define directories for each class\n",
        "    muffin_dir = os.path.join(data_dir, \"muffin\")\n",
        "    chihuahua_dir = os.path.join(data_dir, \"chihuahua\")\n",
        "\n",
        "    # Get image paths for each class (limit to 1000 for demonstration)\n",
        "    muffin_paths = sorted([os.path.join(muffin_dir, f) for f in os.listdir(muffin_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])[:1000]\n",
        "    chihuahua_paths = sorted([os.path.join(chihuahua_dir, f) for f in os.listdir(chihuahua_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])[:1000]\n",
        "\n",
        "    # Create labels (0 for muffin, 1 for chihuahua)\n",
        "    muffin_labels = [0] * len(muffin_paths)\n",
        "    chihuahua_labels = [1] * len(chihuahua_paths)\n",
        "\n",
        "    # Combine paths and labels\n",
        "    all_paths = muffin_paths + chihuahua_paths\n",
        "    all_labels = muffin_labels + chihuahua_labels\n",
        "\n",
        "    # Split into train, validation, and test sets using stratified splitting\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_paths, all_labels, test_size=test_size, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Adjust validation size based on the remaining data after test split\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, stratify=y_temp, random_state=42\n",
        "    )\n",
        "\n",
        "    # Get image transformations\n",
        "    train_transform, val_test_transform = get_transforms()\n",
        "\n",
        "    # Create custom datasets for each set\n",
        "    train_dataset = SplitImageDataset(X_train, y_train, transform=train_transform)\n",
        "    val_dataset = SplitImageDataset(X_val, y_val, transform=val_test_transform)\n",
        "    test_dataset = SplitImageDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "    # Create DataLoaders for efficient batch processing\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, X_test, y_test"
      ],
      "metadata": {
        "id": "9P3p4RE5Uw3V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the Convolutional Neural Network (CNN) model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, architecture='simple', dropout_rate=0.5, activation='relu',\n",
        "                 use_batch_norm=False, kernel_size=3, num_filters=32, pool_size=2):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Define activation function based on the chosen option\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU()\n",
        "        else:\n",
        "            self.activation = nn.ReLU()  # Default to ReLU\n",
        "\n",
        "        # Define architecture configurations for simple, medium, and complex models\n",
        "        configs = {\n",
        "            'simple': {\n",
        "                'conv_layers': 1,\n",
        "                'dense_layers': [50]\n",
        "            },\n",
        "            'medium': {\n",
        "                'conv_layers': 2,\n",
        "                'dense_layers': [100]\n",
        "            },\n",
        "            'complex': {\n",
        "                'conv_layers': 3,\n",
        "                'dense_layers': [200, 100]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Get the configuration for the chosen architecture\n",
        "        config = configs.get(architecture, configs['simple'])\n",
        "\n",
        "        # Build the convolutional block\n",
        "        layers = []\n",
        "        in_channels = 3  # Input channels for RGB images\n",
        "\n",
        "        for i in range(config['conv_layers']):\n",
        "            out_channels = num_filters * (2**i) # Increase filters in deeper layers\n",
        "\n",
        "            # Add Convolutional layer\n",
        "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1))\n",
        "\n",
        "            # Add Batch Normalization if enabled\n",
        "            if use_batch_norm:\n",
        "                layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "            # Add Activation function\n",
        "            layers.append(self.activation)\n",
        "\n",
        "            # Add Max Pooling layer\n",
        "            layers.append(nn.MaxPool2d(pool_size))\n",
        "\n",
        "            # Add Dropout after the last convolutional layer if enabled\n",
        "            if i == config['conv_layers'] - 1 and dropout_rate > 0:\n",
        "                layers.append(nn.Dropout2d(dropout_rate))\n",
        "\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.conv_block = nn.Sequential(*layers)\n",
        "\n",
        "        # Calculate the size of the flattened output after convolutions\n",
        "        # This assumes an initial image size of 224x224 and that each pooling reduces size by pool_size\n",
        "        feature_size = 224 // (pool_size ** config['conv_layers'])\n",
        "        flattened_size = in_channels * feature_size * feature_size\n",
        "\n",
        "        # Build the dense (fully connected) layers\n",
        "        dense_layers = []\n",
        "        in_features = flattened_size\n",
        "\n",
        "        for units in config['dense_layers']:\n",
        "            dense_layers.append(nn.Linear(in_features, units))\n",
        "            dense_layers.append(self.activation)\n",
        "            # Add Dropout to dense layers if enabled\n",
        "            if dropout_rate > 0:\n",
        "                dense_layers.append(nn.Dropout(dropout_rate))\n",
        "            in_features = units\n",
        "\n",
        "        # Add the output layer (2 units for two classes: muffin and chihuahua)\n",
        "        dense_layers.append(nn.Linear(in_features, 2))\n",
        "\n",
        "        self.classifier = nn.Sequential(*dense_layers)\n",
        "\n",
        "    # Define the forward pass of the model\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)       # Pass input through convolutional block\n",
        "        x = torch.flatten(x, 1)    # Flatten the output for dense layers\n",
        "        x = self.classifier(x)       # Pass flattened output through dense layers\n",
        "        return x"
      ],
      "metadata": {
        "id": "m1bii6qgXZ5q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion,\n",
        "                epochs=HYPERPARAMS['epochs'], patience=HYPERPARAMS['patience']):\n",
        "    # Move the model to the appropriate device (GPU or CPU)\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_accuracy = 0.0  # Keep track of the best validation accuracy\n",
        "    epochs_no_improve = 0    # Counter for early stopping\n",
        "\n",
        "    train_losses = []       # List to store training losses per epoch\n",
        "    train_accuracies = []   # List to store training accuracies per epoch\n",
        "    val_losses = []         # List to store validation losses per epoch\n",
        "    val_accuracies = []     # List to store validation accuracies per epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()        # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update training loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average training loss and accuracy for the epoch\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_accuracy = correct / total\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()         # Set model to evaluation mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Disable gradient calculation during validation\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                # Move data to the device\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Update validation loss\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average validation loss and accuracy for the epoch\n",
        "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_val_accuracy = correct / total\n",
        "\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f'Epoch {epoch+1}/{epochs} | '\n",
        "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_accuracy:.4f} | '\n",
        "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if epoch_val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_accuracy\n",
        "            epochs_no_improve = 0\n",
        "            # Save the model state dictionary if it has the best validation accuracy\n",
        "            torch.save(model.state_dict(), f'{output_dir}/best_model.pth')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f'Early stopping after {epoch+1} epochs')\n",
        "                break\n",
        "\n",
        "    # Load the state dictionary of the best model after training\n",
        "    model.load_state_dict(torch.load(f'{output_dir}/best_model.pth'))\n",
        "\n",
        "    # Store training history\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'train_accuracy': train_accuracies,\n",
        "        'val_loss': val_losses,\n",
        "        'val_accuracy': val_accuracies\n",
        "    }\n",
        "\n",
        "    return model, history"
      ],
      "metadata": {
        "id": "Bdng6J2CXhIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to evaluate the model on the test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    model.to(device) # Move model to the device\n",
        "\n",
        "    all_predicted = [] # List to store all predicted labels\n",
        "    all_labels = []    # List to store all actual labels\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # Use Cross-Entropy Loss for evaluation\n",
        "\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            # Move data to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update test loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate test accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Store predicted and actual labels\n",
        "            all_predicted.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    test_loss = running_loss / len(test_loader.dataset)\n",
        "    test_accuracy = correct / total\n",
        "\n",
        "    return test_loss, test_accuracy, all_predicted, all_labels"
      ],
      "metadata": {
        "id": "o7uuhjBBXjZF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot training and validation results (accuracy and loss)\n",
        "def plot_results(history, title=\"\"):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history['train_accuracy'], label='Training accuracy')\n",
        "    ax1.plot(history['val_accuracy'], label='Validation accuracy')\n",
        "    ax1.set_title(f'Model accuracy {title}')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history['train_loss'], label='Training loss')\n",
        "    ax2.plot(history['val_loss'], label='Validation loss')\n",
        "    ax2.set_title(f'Model loss {title}')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "yb0EXXeqXlh4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to plot the confusion matrix and print classification report\n",
        "def plot_confusion_matrix(y_true, y_pred, title=\"\"):\n",
        "    # Calculate the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot the confusion matrix using seaborn heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') # annot=True shows values, fmt='d' formats as integers\n",
        "    plt.title(f'Confusion matrix {title}')\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.ylabel('Actual class')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(y_true, y_pred, target_names=['Muffin', 'Chihuahua']))"
      ],
      "metadata": {
        "id": "PJrON_kbXmDj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to save a sample of test predictions to a CSV file\n",
        "def save_sample_predictions(X_test, y_test, y_pred, save_path):\n",
        "    # Select 30 samples randomly, ensuring representation from both classes\n",
        "    true_labels = np.array(y_test)\n",
        "    predicted_labels = np.array(y_pred)\n",
        "\n",
        "    # Find indices for each class in the test set\n",
        "    muffin_indices = np.where(true_labels == 0)[0]\n",
        "    chihuahua_indices = np.where(true_labels == 1)[0]\n",
        "\n",
        "    # Select a specified number of samples from each class\n",
        "    num_muffin = min(15, len(muffin_indices))\n",
        "    num_chihuahua = min(15, len(chihuahua_indices))\n",
        "\n",
        "    # Randomly select indices from each class\n",
        "    selected_muffin = np.random.choice(muffin_indices, num_muffin, replace=False)\n",
        "    selected_chihuahua = np.random.choice(chihuahua_indices, num_chihuahua, replace=False)\n",
        "\n",
        "    # Combine the selected indices\n",
        "    selected_indices = np.concatenate([selected_muffin, selected_chihuahua])\n",
        "\n",
        "    # Prepare data for CSV\n",
        "    csv_headers = [\"Index\", \"Image Path\", \"Actual Label\", \"Predicted Label\"]\n",
        "\n",
        "    csv_rows = [csv_headers]\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        # Convert numerical labels back to class names\n",
        "        actual = \"Muffin\" if true_labels[idx] == 0 else \"Chihuahua\"\n",
        "        predicted = \"Muffin\" if predicted_labels[idx] == 0 else \"Chihuahua\"\n",
        "        row = [i, X_test[idx], actual, predicted]\n",
        "        csv_rows.append(row)\n",
        "\n",
        "    # Save to CSV file\n",
        "    with open(save_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(csv_rows)\n",
        "\n",
        "    print(f\"Sample predictions saved to: {save_path}\")"
      ],
      "metadata": {
        "id": "JcAt0ceJXn3a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to experiment with different hyperparameter values\n",
        "def experiment_hyperparameter(\n",
        "    train_loader, val_loader, param_name, param_values,\n",
        "    architecture=HYPERPARAMS['architecture'],\n",
        "    dropout_rate=HYPERPARAMS['dropout_rate'],\n",
        "    use_batch_norm=HYPERPARAMS['use_batch_norm'],\n",
        "    activation=HYPERPARAMS['activation'],\n",
        "    optimizer_name=HYPERPARAMS['optimizer_name'],\n",
        "    lr=HYPERPARAMS['learning_rate'],\n",
        "    epochs=HYPERPARAMS['epochs'],\n",
        "    batch_size=HYPERPARAMS['batch_size'],\n",
        "    kernel_size=HYPERPARAMS['kernel_size'],\n",
        "    num_filters=HYPERPARAMS['num_filters'],\n",
        "    pool_size=HYPERPARAMS['pool_size']\n",
        "):\n",
        "    # Ensure the parameter name is supported for experimentation\n",
        "    assert param_name in ['architecture', 'dropout_rate', 'use_batch_norm', 'activation', 'optimizer_name'], \"Unsupported parameter\"\n",
        "\n",
        "    results = {} # Dictionary to store results for each parameter value\n",
        "\n",
        "    for val in param_values:\n",
        "        print(f\"Training with {param_name}={val}...\")\n",
        "\n",
        "        # Create a new model instance with the current parameter value\n",
        "        model_kwargs = {\n",
        "            'architecture': architecture,\n",
        "            'dropout_rate': dropout_rate,\n",
        "            'use_batch_norm': use_batch_norm,\n",
        "            'activation': activation,\n",
        "            'kernel_size': kernel_size,\n",
        "            'num_filters': num_filters,\n",
        "            'pool_size': pool_size\n",
        "        }\n",
        "\n",
        "        # Update the specific parameter being experimented\n",
        "        if param_name != 'optimizer_name': # Optimizer is not a model parameter\n",
        "            model_kwargs[param_name] = val\n",
        "\n",
        "        model = CNN(**model_kwargs)\n",
        "\n",
        "        # Select the optimizer based on the parameter being experimented or the default\n",
        "        curr_optimizer_name = optimizer_name\n",
        "        if param_name == 'optimizer_name':\n",
        "            curr_optimizer_name = val\n",
        "\n",
        "        if curr_optimizer_name == 'adam':\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        elif curr_optimizer_name == 'sgd':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "        elif curr_optimizer_name == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)  # Default to Adam\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss() # Loss function\n",
        "\n",
        "        # Train the model with the current parameter value\n",
        "        _, history = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=epochs)\n",
        "\n",
        "        # Store the training history and best validation metrics\n",
        "        results[val] = {\n",
        "            'history': history,\n",
        "            'val_accuracy': max(history['val_accuracy']), # Best validation accuracy achieved\n",
        "            'val_loss': min(history['val_loss']),       # Minimum validation loss achieved\n",
        "        }\n",
        "\n",
        "    # Plot the validation accuracy and loss for each parameter value\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for val in param_values:\n",
        "        plt.plot(results[val]['history']['val_accuracy'], label=f'{param_name}={val}')\n",
        "    plt.title(f'Validation accuracy comparison ({param_name})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for val in param_values:\n",
        "        plt.plot(results[val]['history']['val_loss'], label=f'{param_name}={val}')\n",
        "    plt.title(f'Validation loss comparison ({param_name})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find and print the parameter value that resulted in the best validation accuracy\n",
        "    best_val = max(results, key=lambda x: results[x]['val_accuracy'])\n",
        "    print(f\"Best {param_name}: {best_val}\")\n",
        "    print(f\"Validation accuracy: {results[best_val]['val_accuracy']:.4f}\")\n",
        "    print(f\"Validation loss: {results[best_val]['val_loss']:.4f}\")\n",
        "\n",
        "    return results, best_val"
      ],
      "metadata": {
        "id": "GjgNIdC1XqVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to create and evaluate the final best model based on experiment results\n",
        "def create_best_model(train_loader, val_loader, test_loader, X_test, y_test,\n",
        "                      architecture, dropout_rate, use_batch_norm, activation, optimizer_name, lr=HYPERPARAMS['learning_rate']):\n",
        "    # Create the best model with the optimal hyperparameters found\n",
        "    best_model = CNN(\n",
        "        architecture=architecture,\n",
        "        dropout_rate=dropout_rate,\n",
        "        use_batch_norm=use_batch_norm,\n",
        "        activation=activation\n",
        "    )\n",
        "\n",
        "    # Select the best optimizer\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = optim.Adam(best_model.parameters(), lr=lr)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = optim.SGD(best_model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(best_model.parameters(), lr=lr)\n",
        "    else:\n",
        "        optimizer = optim.Adam(best_model.parameters(), lr=lr) # Default\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # Loss function\n",
        "\n",
        "    # Train the best model\n",
        "    best_model, history = train_model(best_model, train_loader, val_loader, optimizer, criterion)\n",
        "\n",
        "    # Plot the training and validation results for the best model\n",
        "    plot_results(history, f\"(Best model: {architecture}, {activation}, {optimizer_name})\")\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    test_loss, test_accuracy, y_pred, y_true = evaluate_model(best_model, test_loader)\n",
        "    print(f\"Testing accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Testing loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Plot the confusion matrix for the test set\n",
        "    plot_confusion_matrix(y_true, y_pred, \"Best model\")\n",
        "\n",
        "    # Save a sample of the test predictions\n",
        "    save_sample_predictions(X_test, y_test, y_pred, f\"{output_dir}/test_predictions.csv\")\n",
        "\n",
        "    return best_model, history"
      ],
      "metadata": {
        "id": "ecr2SubvXttH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader, X_test, y_test = prepare_data(\n",
        "        data_dir=data_dir,\n",
        "        batch_size=HYPERPARAMS['batch_size'],\n",
        "        test_size=HYPERPARAMS['test_size'],\n",
        "        val_size=HYPERPARAMS['val_size']\n",
        "    )\n",
        "\n",
        "    print(f\"Number of training batches: {len(train_loader)}\")\n",
        "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "    print(f\"Number of testing batches: {len(test_loader)}\")\n",
        "\n",
        "    # Experiment 1: Architecture comparison\n",
        "    print(\"\\nEXPERIMENT NR.1: ARCHITECTURE COMPARISON\")\n",
        "    arch_results, best_arch = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        param_name='architecture',\n",
        "        param_values=['simple', 'medium', 'complex']\n",
        "    )\n",
        "\n",
        "    # Experiment 2: Dropout layers comparison\n",
        "    print(\"\\nEXPERIMENT NR.2: DROPOUT LAYERS COMPARISON\")\n",
        "    dropout_results, best_dropout = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        param_name='dropout_rate',\n",
        "        param_values=[0.0, 0.2, 0.5, 0.7]\n",
        "    )\n",
        "\n",
        "    # Experiment 3: Batch normalization comparison\n",
        "    print(\"\\nEXPERIMENT NR.3: BATCH NORMALISATION COMPARISON\")\n",
        "    bn_results, best_bn = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        dropout_rate=best_dropout,\n",
        "        param_name='use_batch_norm',\n",
        "        param_values=[False, True]\n",
        "    )\n",
        "\n",
        "    # Experiment 4: Activation functions comparison\n",
        "    print(\"\\nEXPERIMENT NR.4: ACTIVATION FUNCTIONS COMPARISON\")\n",
        "    act_results, best_activation = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        dropout_rate=best_dropout,\n",
        "        use_batch_norm=best_bn,\n",
        "        param_name='activation',\n",
        "        param_values=['relu', 'tanh', 'elu', 'selu']\n",
        "    )\n",
        "\n",
        "    # Experiment 5: Optimizer comparison\n",
        "    print(\"\\nEXPERIMENT NR.5: OPTIMIZER COMPARISON\")\n",
        "    opt_results, best_optimizer = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        dropout_rate=best_dropout,\n",
        "        use_batch_norm=best_bn,\n",
        "        activation=best_activation,\n",
        "        param_name='optimizer_name',\n",
        "        param_values=['adam', 'sgd', 'rmsprop']\n",
        "    )\n",
        "\n",
        "    # Create and evaluate best model\n",
        "    print(\"\\nBEST MODEL TRAINING AND TESTING\")\n",
        "    best_model, best_history = create_best_model(\n",
        "        train_loader, val_loader, test_loader, X_test, y_test,\n",
        "        best_arch, best_dropout, best_bn, best_activation, best_optimizer\n",
        "    )\n",
        "\n",
        "    # Print best hyperparameters\n",
        "    print(\"\\nBEST HYPERPARAMETERS:\")\n",
        "    print(f\"Architecture: {best_arch}\")\n",
        "    print(f\"Dropout value: {best_dropout}\")\n",
        "    print(f\"Batch normalization: {best_bn}\")\n",
        "    print(f\"Activation function: {best_activation}\")\n",
        "    print(f\"Optimizer: {best_optimizer}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RWnq2VT_Xytl",
        "outputId": "8e3a108f-7f25-4a3c-84eb-8cce11f57d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of training batches: 80\n",
            "Number of validation batches: 10\n",
            "Number of testing batches: 10\n",
            "\n",
            "EXPERIMENT NR.4: ACTIVATION FUNCTIONS COMPARISON\n",
            "Training with activation=relu...\n",
            "Epoch 1/50 | Train Loss: 0.5922 | Train Acc: 0.7025 | Val Loss: 0.4961 | Val Acc: 0.7450\n",
            "Epoch 2/50 | Train Loss: 0.4633 | Train Acc: 0.7944 | Val Loss: 0.3956 | Val Acc: 0.8650\n",
            "Epoch 3/50 | Train Loss: 0.3824 | Train Acc: 0.8481 | Val Loss: 0.3550 | Val Acc: 0.8750\n",
            "Epoch 4/50 | Train Loss: 0.3208 | Train Acc: 0.8706 | Val Loss: 0.3074 | Val Acc: 0.8800\n",
            "Epoch 5/50 | Train Loss: 0.3023 | Train Acc: 0.8838 | Val Loss: 0.2740 | Val Acc: 0.9000\n",
            "Epoch 6/50 | Train Loss: 0.2732 | Train Acc: 0.8950 | Val Loss: 0.2531 | Val Acc: 0.9000\n",
            "Epoch 7/50 | Train Loss: 0.2307 | Train Acc: 0.9087 | Val Loss: 0.2391 | Val Acc: 0.9100\n",
            "Epoch 8/50 | Train Loss: 0.2069 | Train Acc: 0.9175 | Val Loss: 0.2403 | Val Acc: 0.9250\n",
            "Epoch 9/50 | Train Loss: 0.1966 | Train Acc: 0.9213 | Val Loss: 0.1887 | Val Acc: 0.9250\n",
            "Epoch 10/50 | Train Loss: 0.1792 | Train Acc: 0.9287 | Val Loss: 0.2456 | Val Acc: 0.9150\n",
            "Epoch 11/50 | Train Loss: 0.1816 | Train Acc: 0.9306 | Val Loss: 0.2316 | Val Acc: 0.9250\n",
            "Epoch 12/50 | Train Loss: 0.1749 | Train Acc: 0.9369 | Val Loss: 0.2305 | Val Acc: 0.9250\n",
            "Epoch 13/50 | Train Loss: 0.1609 | Train Acc: 0.9387 | Val Loss: 0.2494 | Val Acc: 0.8900\n",
            "Epoch 14/50 | Train Loss: 0.1328 | Train Acc: 0.9463 | Val Loss: 0.2008 | Val Acc: 0.9250\n",
            "Epoch 15/50 | Train Loss: 0.1252 | Train Acc: 0.9519 | Val Loss: 0.3081 | Val Acc: 0.8900\n",
            "Epoch 16/50 | Train Loss: 0.1309 | Train Acc: 0.9450 | Val Loss: 0.2549 | Val Acc: 0.9150\n",
            "Epoch 17/50 | Train Loss: 0.1113 | Train Acc: 0.9619 | Val Loss: 0.2645 | Val Acc: 0.9200\n",
            "Epoch 18/50 | Train Loss: 0.1198 | Train Acc: 0.9575 | Val Loss: 0.2383 | Val Acc: 0.9150\n",
            "Early stopping after 18 epochs\n",
            "Training with activation=tanh...\n",
            "Epoch 1/50 | Train Loss: 0.7156 | Train Acc: 0.5444 | Val Loss: 0.6364 | Val Acc: 0.6350\n",
            "Epoch 2/50 | Train Loss: 0.6206 | Train Acc: 0.6488 | Val Loss: 0.5895 | Val Acc: 0.6250\n",
            "Epoch 3/50 | Train Loss: 0.5870 | Train Acc: 0.6887 | Val Loss: 0.5662 | Val Acc: 0.7200\n",
            "Epoch 4/50 | Train Loss: 0.5663 | Train Acc: 0.7150 | Val Loss: 0.5214 | Val Acc: 0.7150\n",
            "Epoch 5/50 | Train Loss: 0.5549 | Train Acc: 0.7150 | Val Loss: 0.6497 | Val Acc: 0.6200\n",
            "Epoch 6/50 | Train Loss: 0.6797 | Train Acc: 0.5763 | Val Loss: 0.6263 | Val Acc: 0.6300\n",
            "Epoch 7/50 | Train Loss: 0.6411 | Train Acc: 0.6188 | Val Loss: 0.6075 | Val Acc: 0.6300\n",
            "Epoch 8/50 | Train Loss: 0.6228 | Train Acc: 0.6469 | Val Loss: 0.5093 | Val Acc: 0.7500\n",
            "Epoch 9/50 | Train Loss: 0.5582 | Train Acc: 0.7206 | Val Loss: 0.5583 | Val Acc: 0.7200\n",
            "Epoch 10/50 | Train Loss: 0.5493 | Train Acc: 0.7275 | Val Loss: 0.5127 | Val Acc: 0.7650\n",
            "Epoch 11/50 | Train Loss: 0.5373 | Train Acc: 0.7325 | Val Loss: 0.5095 | Val Acc: 0.7450\n",
            "Epoch 12/50 | Train Loss: 0.5585 | Train Acc: 0.7188 | Val Loss: 0.5152 | Val Acc: 0.7500\n",
            "Epoch 13/50 | Train Loss: 0.6071 | Train Acc: 0.6631 | Val Loss: 0.5982 | Val Acc: 0.6950\n",
            "Epoch 14/50 | Train Loss: 0.6011 | Train Acc: 0.6819 | Val Loss: 0.5562 | Val Acc: 0.6550\n",
            "Epoch 15/50 | Train Loss: 0.5976 | Train Acc: 0.6775 | Val Loss: 0.5579 | Val Acc: 0.7450\n",
            "Epoch 16/50 | Train Loss: 0.5907 | Train Acc: 0.6837 | Val Loss: 0.5627 | Val Acc: 0.7650\n",
            "Epoch 17/50 | Train Loss: 0.6096 | Train Acc: 0.6531 | Val Loss: 0.5524 | Val Acc: 0.7800\n",
            "Epoch 18/50 | Train Loss: 0.5723 | Train Acc: 0.6900 | Val Loss: 0.5096 | Val Acc: 0.7950\n",
            "Epoch 19/50 | Train Loss: 0.5521 | Train Acc: 0.7181 | Val Loss: 0.5130 | Val Acc: 0.7450\n",
            "Epoch 20/50 | Train Loss: 0.5501 | Train Acc: 0.7262 | Val Loss: 0.4891 | Val Acc: 0.7550\n",
            "Epoch 21/50 | Train Loss: 0.5484 | Train Acc: 0.7319 | Val Loss: 0.4852 | Val Acc: 0.7800\n",
            "Epoch 22/50 | Train Loss: 0.5321 | Train Acc: 0.7356 | Val Loss: 0.5053 | Val Acc: 0.7350\n",
            "Epoch 23/50 | Train Loss: 0.5490 | Train Acc: 0.7238 | Val Loss: 0.5096 | Val Acc: 0.7600\n",
            "Epoch 24/50 | Train Loss: 0.5122 | Train Acc: 0.7569 | Val Loss: 0.5570 | Val Acc: 0.7350\n",
            "Epoch 25/50 | Train Loss: 0.5329 | Train Acc: 0.7344 | Val Loss: 0.5211 | Val Acc: 0.7500\n",
            "Epoch 26/50 | Train Loss: 0.5441 | Train Acc: 0.7219 | Val Loss: 0.5964 | Val Acc: 0.6800\n",
            "Epoch 27/50 | Train Loss: 0.5504 | Train Acc: 0.7262 | Val Loss: 0.4957 | Val Acc: 0.7650\n",
            "Epoch 28/50 | Train Loss: 0.4931 | Train Acc: 0.7712 | Val Loss: 0.4977 | Val Acc: 0.7750\n",
            "Early stopping after 28 epochs\n",
            "Training with activation=elu...\n",
            "Epoch 1/50 | Train Loss: 0.7879 | Train Acc: 0.6844 | Val Loss: 0.3704 | Val Acc: 0.8300\n",
            "Epoch 2/50 | Train Loss: 0.4786 | Train Acc: 0.7825 | Val Loss: 0.3288 | Val Acc: 0.8650\n",
            "Epoch 3/50 | Train Loss: 0.4172 | Train Acc: 0.8187 | Val Loss: 0.4488 | Val Acc: 0.7700\n",
            "Epoch 4/50 | Train Loss: 0.4129 | Train Acc: 0.8294 | Val Loss: 0.2758 | Val Acc: 0.8750\n",
            "Epoch 5/50 | Train Loss: 0.3323 | Train Acc: 0.8631 | Val Loss: 0.2386 | Val Acc: 0.9100\n",
            "Epoch 6/50 | Train Loss: 0.3266 | Train Acc: 0.8725 | Val Loss: 0.2897 | Val Acc: 0.8850\n",
            "Epoch 7/50 | Train Loss: 0.3067 | Train Acc: 0.8819 | Val Loss: 0.2544 | Val Acc: 0.8750\n",
            "Epoch 8/50 | Train Loss: 0.2862 | Train Acc: 0.8994 | Val Loss: 0.3454 | Val Acc: 0.8550\n",
            "Epoch 9/50 | Train Loss: 0.2520 | Train Acc: 0.8994 | Val Loss: 0.2597 | Val Acc: 0.9050\n",
            "Epoch 10/50 | Train Loss: 0.2622 | Train Acc: 0.9031 | Val Loss: 0.2429 | Val Acc: 0.8900\n",
            "Epoch 11/50 | Train Loss: 0.2568 | Train Acc: 0.9000 | Val Loss: 0.2353 | Val Acc: 0.9000\n",
            "Epoch 12/50 | Train Loss: 0.2261 | Train Acc: 0.9125 | Val Loss: 0.2688 | Val Acc: 0.9150\n",
            "Epoch 13/50 | Train Loss: 0.2165 | Train Acc: 0.9231 | Val Loss: 0.2816 | Val Acc: 0.8800\n",
            "Epoch 14/50 | Train Loss: 0.2103 | Train Acc: 0.9194 | Val Loss: 0.2132 | Val Acc: 0.9300\n",
            "Epoch 15/50 | Train Loss: 0.1843 | Train Acc: 0.9263 | Val Loss: 0.2302 | Val Acc: 0.9300\n",
            "Epoch 16/50 | Train Loss: 0.1577 | Train Acc: 0.9406 | Val Loss: 0.2224 | Val Acc: 0.9450\n",
            "Epoch 17/50 | Train Loss: 0.1503 | Train Acc: 0.9475 | Val Loss: 0.2522 | Val Acc: 0.9300\n",
            "Epoch 18/50 | Train Loss: 0.1615 | Train Acc: 0.9437 | Val Loss: 0.2467 | Val Acc: 0.9200\n",
            "Epoch 19/50 | Train Loss: 0.1448 | Train Acc: 0.9494 | Val Loss: 0.3153 | Val Acc: 0.9250\n",
            "Epoch 20/50 | Train Loss: 0.1270 | Train Acc: 0.9481 | Val Loss: 0.2726 | Val Acc: 0.9400\n",
            "Epoch 21/50 | Train Loss: 0.1298 | Train Acc: 0.9525 | Val Loss: 0.2999 | Val Acc: 0.9150\n",
            "Epoch 22/50 | Train Loss: 0.1458 | Train Acc: 0.9487 | Val Loss: 0.2448 | Val Acc: 0.9100\n",
            "Epoch 23/50 | Train Loss: 0.1571 | Train Acc: 0.9444 | Val Loss: 0.3944 | Val Acc: 0.9000\n",
            "Epoch 24/50 | Train Loss: 0.1328 | Train Acc: 0.9469 | Val Loss: 0.3787 | Val Acc: 0.8950\n",
            "Epoch 25/50 | Train Loss: 0.1246 | Train Acc: 0.9506 | Val Loss: 0.3967 | Val Acc: 0.9000\n",
            "Epoch 26/50 | Train Loss: 0.1070 | Train Acc: 0.9650 | Val Loss: 0.2800 | Val Acc: 0.9350\n",
            "Early stopping after 26 epochs\n",
            "Training with activation=selu...\n",
            "Epoch 1/50 | Train Loss: 0.8840 | Train Acc: 0.7056 | Val Loss: 0.4593 | Val Acc: 0.7700\n",
            "Epoch 2/50 | Train Loss: 0.5298 | Train Acc: 0.7756 | Val Loss: 0.6213 | Val Acc: 0.7750\n",
            "Epoch 3/50 | Train Loss: 0.5131 | Train Acc: 0.7950 | Val Loss: 0.3367 | Val Acc: 0.8600\n",
            "Epoch 4/50 | Train Loss: 0.4186 | Train Acc: 0.8306 | Val Loss: 0.3295 | Val Acc: 0.8450\n",
            "Epoch 5/50 | Train Loss: 0.4072 | Train Acc: 0.8213 | Val Loss: 0.3163 | Val Acc: 0.8600\n",
            "Epoch 6/50 | Train Loss: 0.3797 | Train Acc: 0.8462 | Val Loss: 0.3296 | Val Acc: 0.8650\n",
            "Epoch 7/50 | Train Loss: 0.3537 | Train Acc: 0.8488 | Val Loss: 0.3209 | Val Acc: 0.8700\n",
            "Epoch 8/50 | Train Loss: 0.3317 | Train Acc: 0.8594 | Val Loss: 0.3566 | Val Acc: 0.8100\n",
            "Epoch 9/50 | Train Loss: 0.3424 | Train Acc: 0.8612 | Val Loss: 0.3145 | Val Acc: 0.8650\n",
            "Epoch 10/50 | Train Loss: 0.2893 | Train Acc: 0.8938 | Val Loss: 0.2641 | Val Acc: 0.9000\n",
            "Epoch 11/50 | Train Loss: 0.2542 | Train Acc: 0.9038 | Val Loss: 0.2511 | Val Acc: 0.8950\n",
            "Epoch 12/50 | Train Loss: 0.2502 | Train Acc: 0.8969 | Val Loss: 0.2890 | Val Acc: 0.8900\n",
            "Epoch 13/50 | Train Loss: 0.2335 | Train Acc: 0.9119 | Val Loss: 0.3006 | Val Acc: 0.9050\n",
            "Epoch 14/50 | Train Loss: 0.2550 | Train Acc: 0.8956 | Val Loss: 0.2947 | Val Acc: 0.8750\n",
            "Epoch 15/50 | Train Loss: 0.2298 | Train Acc: 0.8950 | Val Loss: 0.2702 | Val Acc: 0.8950\n",
            "Epoch 16/50 | Train Loss: 0.2288 | Train Acc: 0.9044 | Val Loss: 0.3191 | Val Acc: 0.8650\n",
            "Epoch 17/50 | Train Loss: 0.2317 | Train Acc: 0.9062 | Val Loss: 0.2790 | Val Acc: 0.9050\n",
            "Epoch 18/50 | Train Loss: 0.2021 | Train Acc: 0.9206 | Val Loss: 0.3167 | Val Acc: 0.8800\n",
            "Epoch 19/50 | Train Loss: 0.2025 | Train Acc: 0.9206 | Val Loss: 0.3536 | Val Acc: 0.9100\n",
            "Epoch 20/50 | Train Loss: 0.1768 | Train Acc: 0.9275 | Val Loss: 0.3549 | Val Acc: 0.9200\n",
            "Epoch 21/50 | Train Loss: 0.1568 | Train Acc: 0.9394 | Val Loss: 0.3315 | Val Acc: 0.9050\n",
            "Epoch 22/50 | Train Loss: 0.1971 | Train Acc: 0.9256 | Val Loss: 0.2767 | Val Acc: 0.9050\n",
            "Epoch 23/50 | Train Loss: 0.1437 | Train Acc: 0.9350 | Val Loss: 0.3711 | Val Acc: 0.8950\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-a58879f8dc46>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0;31m# Experiment 4: Activation functions comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nEXPERIMENT NR.4: ACTIVATION FUNCTIONS COMPARISON\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     act_results, best_activation = experiment_hyperparameter(\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0marchitecture\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbest_arch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-af7c76687df1>\u001b[0m in \u001b[0;36mexperiment_hyperparameter\u001b[0;34m(train_loader, val_loader, param_name, param_values, architecture, dropout_rate, use_batch_norm, activation, optimizer_name, lr, epochs, batch_size, kernel_size, num_filters, pool_size)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;31m# Train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;31m# Store results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-8-8ca3e614bd96>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, optimizer, criterion, epochs, patience)\u001b[0m\n\u001b[1;32m     23\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-7-728522deaac3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_block\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    248\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 554\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    547\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m             )\n\u001b[0;32m--> 549\u001b[0;31m         return F.conv2d(\n\u001b[0m\u001b[1;32m    550\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdilation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroups\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m         )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}
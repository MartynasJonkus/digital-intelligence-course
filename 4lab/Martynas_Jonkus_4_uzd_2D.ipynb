{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ITrL3pjrJRJC"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import os\n",
        "import zipfile\n",
        "import csv\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import ImageFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C5jnUZnSJS18",
        "outputId": "d14d1f02-bc17-46af-f208-eaa5d3ea7048"
      },
      "outputs": [],
      "source": [
        "# Mount Google Drive to access the data and save output\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "data_dir = \"drive/MyDrive/SI_4lab/2D/data\"\n",
        "output_dir = \"drive/MyDrive/SI_4lab/2D\"\n",
        "os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "np.random.seed(69)\n",
        "torch.manual_seed(69)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(69)\n",
        "\n",
        "# Determine the device to use (GPU if available, otherwise CPU)\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSNJklzMYgxy"
      },
      "outputs": [],
      "source": [
        "# Define a dictionary of hyperparameters\n",
        "HYPERPARAMS = {\n",
        "    # Data parameters\n",
        "    'img_size': 224,       # Size to resize images to\n",
        "    'batch_size': 20,      # Number of images per batch\n",
        "    'test_size': 0.1,      # Proportion of data for the test set\n",
        "    'val_size': 0.1,       # Proportion of data for the validation set\n",
        "\n",
        "    # Training parameters\n",
        "    'epochs': 50,          # Maximum number of training epochs\n",
        "    'patience': 10,        # Number of epochs with no improvement before early stopping\n",
        "    'learning_rate': 0.001, # Learning rate for the optimizer\n",
        "\n",
        "    # Model parameters\n",
        "    'architecture': 'simple',  # 'simple', 'medium', or 'complex' CNN architecture\n",
        "    'dropout_rate': 0.5,   # Dropout rate for regularization\n",
        "    'use_batch_norm': False, # Whether to use batch normalization\n",
        "    'activation': 'relu',  # Activation function for hidden layers ('relu', 'tanh', 'elu', or 'selu')\n",
        "    'optimizer_name': 'adam',  # Optimizer to use ('adam', 'sgd', or 'rmsprop')\n",
        "    'kernel_size': 3,      # Kernel size for convolutional layers\n",
        "    'num_filters': 32,     # Base number of filters in the first convolutional layer\n",
        "    'pool_size': 2         # Size of the max pooling window\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sk3N-F-dJavr"
      },
      "outputs": [],
      "source": [
        "# Function to define image transformations for training, validation, and testing\n",
        "def get_transforms(img_size=HYPERPARAMS['img_size']):\n",
        "    # Transformations for the training set (includes data augmentation)\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),  # Resize images\n",
        "        transforms.RandomHorizontalFlip(),       # Randomly flip images horizontally\n",
        "        transforms.RandomRotation(10),          # Randomly rotate images by up to 10 degrees\n",
        "        transforms.ToTensor(),                  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats\n",
        "    ])\n",
        "\n",
        "    # Transformations for validation and test sets (no data augmentation)\n",
        "    val_test_transform = transforms.Compose([\n",
        "        transforms.Resize((img_size, img_size)),  # Resize images\n",
        "        transforms.ToTensor(),                  # Convert images to PyTorch tensors\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]) # Normalize with ImageNet stats\n",
        "    ])\n",
        "\n",
        "    return train_transform, val_test_transform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3uRtTqpxUobM"
      },
      "outputs": [],
      "source": [
        "# Custom PyTorch Dataset for loading images from a list of file paths\n",
        "class SplitImageDataset(Dataset):\n",
        "    def __init__(self, image_paths, labels, transform=None):\n",
        "        self.image_paths = image_paths  # List of image file paths\n",
        "        self.labels = labels            # List of corresponding labels\n",
        "        self.transform = transform      # Image transformations to apply\n",
        "\n",
        "    def __len__(self):\n",
        "        # Return the total number of images in the dataset\n",
        "        return len(self.image_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the image path and label for a given index\n",
        "        img_path = self.image_paths[idx]\n",
        "        image = Image.open(img_path).convert(\"RGB\") # Open and convert image to RGB\n",
        "\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Apply transformations if specified\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9P3p4RE5Uw3V"
      },
      "outputs": [],
      "source": [
        "# Function to prepare the data: load images, split into train/val/test sets, and create DataLoaders\n",
        "def prepare_data(data_dir, batch_size=HYPERPARAMS['batch_size'],\n",
        "                test_size=HYPERPARAMS['test_size'], val_size=HYPERPARAMS['val_size']):\n",
        "    # Define directories for each class\n",
        "    muffin_dir = os.path.join(data_dir, \"muffin\")\n",
        "    chihuahua_dir = os.path.join(data_dir, \"chihuahua\")\n",
        "\n",
        "    # Get image paths for each class (limit to 1000 for demonstration)\n",
        "    muffin_paths = sorted([os.path.join(muffin_dir, f) for f in os.listdir(muffin_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])[:1000]\n",
        "    chihuahua_paths = sorted([os.path.join(chihuahua_dir, f) for f in os.listdir(chihuahua_dir) if f.endswith(('.jpg', '.jpeg', '.png'))])[:1000]\n",
        "\n",
        "    # Create labels (0 for muffin, 1 for chihuahua)\n",
        "    muffin_labels = [0] * len(muffin_paths)\n",
        "    chihuahua_labels = [1] * len(chihuahua_paths)\n",
        "\n",
        "    # Combine paths and labels\n",
        "    all_paths = muffin_paths + chihuahua_paths\n",
        "    all_labels = muffin_labels + chihuahua_labels\n",
        "\n",
        "    # Split into train, validation, and test sets using stratified splitting\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(\n",
        "        all_paths, all_labels, test_size=test_size, stratify=all_labels, random_state=42\n",
        "    )\n",
        "\n",
        "    # Adjust validation size based on the remaining data after test split\n",
        "    val_size_adjusted = val_size / (1 - test_size)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X_temp, y_temp, test_size=val_size_adjusted, stratify=y_temp, random_state=42\n",
        "    )\n",
        "\n",
        "    # Get image transformations\n",
        "    train_transform, val_test_transform = get_transforms()\n",
        "\n",
        "    # Create custom datasets for each set\n",
        "    train_dataset = SplitImageDataset(X_train, y_train, transform=train_transform)\n",
        "    val_dataset = SplitImageDataset(X_val, y_val, transform=val_test_transform)\n",
        "    test_dataset = SplitImageDataset(X_test, y_test, transform=val_test_transform)\n",
        "\n",
        "    # Create DataLoaders for efficient batch processing\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n",
        "    return train_loader, val_loader, test_loader, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m1bii6qgXZ5q"
      },
      "outputs": [],
      "source": [
        "# Define the Convolutional Neural Network (CNN) model\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self, architecture='simple', dropout_rate=0.5, activation='relu',\n",
        "                 use_batch_norm=False, kernel_size=3, num_filters=32, pool_size=2):\n",
        "        super(CNN, self).__init__()\n",
        "\n",
        "        # Define activation function based on the chosen option\n",
        "        if activation == 'relu':\n",
        "            self.activation = nn.ReLU()\n",
        "        elif activation == 'tanh':\n",
        "            self.activation = nn.Tanh()\n",
        "        elif activation == 'elu':\n",
        "            self.activation = nn.ELU()\n",
        "        elif activation == 'selu':\n",
        "            self.activation = nn.SELU()\n",
        "        else:\n",
        "            self.activation = nn.ReLU()  # Default to ReLU\n",
        "\n",
        "        # Define architecture configurations for simple, medium, and complex models\n",
        "        configs = {\n",
        "            'simple': {\n",
        "                'conv_layers': 1,\n",
        "                'dense_layers': [50]\n",
        "            },\n",
        "            'medium': {\n",
        "                'conv_layers': 2,\n",
        "                'dense_layers': [100]\n",
        "            },\n",
        "            'complex': {\n",
        "                'conv_layers': 3,\n",
        "                'dense_layers': [200, 100]\n",
        "            }\n",
        "        }\n",
        "\n",
        "        # Get the configuration for the chosen architecture\n",
        "        config = configs.get(architecture, configs['simple'])\n",
        "\n",
        "        # Build the convolutional block\n",
        "        layers = []\n",
        "        in_channels = 3  # Input channels for RGB images\n",
        "\n",
        "        for i in range(config['conv_layers']):\n",
        "            out_channels = num_filters * (2**i) # Increase filters in deeper layers\n",
        "\n",
        "            # Add Convolutional layer\n",
        "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, padding=1))\n",
        "\n",
        "            # Add Batch Normalization if enabled\n",
        "            if use_batch_norm:\n",
        "                layers.append(nn.BatchNorm2d(out_channels))\n",
        "\n",
        "            # Add Activation function\n",
        "            layers.append(self.activation)\n",
        "\n",
        "            # Add Max Pooling layer\n",
        "            layers.append(nn.MaxPool2d(pool_size))\n",
        "\n",
        "            # Add Dropout after the last convolutional layer if enabled\n",
        "            if i == config['conv_layers'] - 1 and dropout_rate > 0:\n",
        "                layers.append(nn.Dropout2d(dropout_rate))\n",
        "\n",
        "            in_channels = out_channels\n",
        "\n",
        "        self.conv_block = nn.Sequential(*layers)\n",
        "\n",
        "        # Calculate the size of the flattened output after convolutions\n",
        "        # This assumes an initial image size of 224x224 and that each pooling reduces size by pool_size\n",
        "        feature_size = 224 // (pool_size ** config['conv_layers'])\n",
        "        flattened_size = in_channels * feature_size * feature_size\n",
        "\n",
        "        # Build the dense (fully connected) layers\n",
        "        dense_layers = []\n",
        "        in_features = flattened_size\n",
        "\n",
        "        for units in config['dense_layers']:\n",
        "            dense_layers.append(nn.Linear(in_features, units))\n",
        "            dense_layers.append(self.activation)\n",
        "            # Add Dropout to dense layers if enabled\n",
        "            if dropout_rate > 0:\n",
        "                dense_layers.append(nn.Dropout(dropout_rate))\n",
        "            in_features = units\n",
        "\n",
        "        # Add the output layer (2 units for two classes: muffin and chihuahua)\n",
        "        dense_layers.append(nn.Linear(in_features, 2))\n",
        "\n",
        "        self.classifier = nn.Sequential(*dense_layers)\n",
        "\n",
        "    # Define the forward pass of the model\n",
        "    def forward(self, x):\n",
        "        x = self.conv_block(x)       # Pass input through convolutional block\n",
        "        x = torch.flatten(x, 1)    # Flatten the output for dense layers\n",
        "        x = self.classifier(x)       # Pass flattened output through dense layers\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Bdng6J2CXhIR"
      },
      "outputs": [],
      "source": [
        "# Function to train the model\n",
        "def train_model(model, train_loader, val_loader, optimizer, criterion,\n",
        "                epochs=HYPERPARAMS['epochs'], patience=HYPERPARAMS['patience']):\n",
        "    # Move the model to the appropriate device (GPU or CPU)\n",
        "    model.to(device)\n",
        "\n",
        "    best_val_accuracy = 0.0  # Keep track of the best validation accuracy\n",
        "    epochs_no_improve = 0    # Counter for early stopping\n",
        "\n",
        "    train_losses = []       # List to store training losses per epoch\n",
        "    train_accuracies = []   # List to store training accuracies per epoch\n",
        "    val_losses = []         # List to store validation losses per epoch\n",
        "    val_accuracies = []     # List to store validation accuracies per epoch\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Training phase\n",
        "        model.train()        # Set model to training mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        for images, labels in train_loader:\n",
        "            # Move data to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Zero the gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimization\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update training loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate training accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average training loss and accuracy for the epoch\n",
        "        epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "        epoch_train_accuracy = correct / total\n",
        "\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()         # Set model to evaluation mode\n",
        "        running_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        # Disable gradient calculation during validation\n",
        "        with torch.no_grad():\n",
        "            for images, labels in val_loader:\n",
        "                # Move data to the device\n",
        "                images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(images)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Update validation loss\n",
        "                running_loss += loss.item() * images.size(0)\n",
        "\n",
        "                # Calculate validation accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "\n",
        "        # Calculate average validation loss and accuracy for the epoch\n",
        "        epoch_val_loss = running_loss / len(val_loader.dataset)\n",
        "        epoch_val_accuracy = correct / total\n",
        "\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "        # Print epoch results\n",
        "        print(f'Epoch {epoch+1}/{epochs} | '\n",
        "              f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_accuracy:.4f} | '\n",
        "              f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_accuracy:.4f}')\n",
        "\n",
        "        # Early stopping logic\n",
        "        if epoch_val_accuracy > best_val_accuracy:\n",
        "            best_val_accuracy = epoch_val_accuracy\n",
        "            epochs_no_improve = 0\n",
        "            # Save the model state dictionary if it has the best validation accuracy\n",
        "            torch.save(model.state_dict(), f'{output_dir}/best_model.pth')\n",
        "        else:\n",
        "            epochs_no_improve += 1\n",
        "            if epochs_no_improve >= patience:\n",
        "                print(f'Early stopping after {epoch+1} epochs')\n",
        "                break\n",
        "\n",
        "    # Load the state dictionary of the best model after training\n",
        "    model.load_state_dict(torch.load(f'{output_dir}/best_model.pth'))\n",
        "\n",
        "    # Store training history\n",
        "    history = {\n",
        "        'train_loss': train_losses,\n",
        "        'train_accuracy': train_accuracies,\n",
        "        'val_loss': val_losses,\n",
        "        'val_accuracy': val_accuracies\n",
        "    }\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7uuhjBBXjZF"
      },
      "outputs": [],
      "source": [
        "# Function to evaluate the model on the test set\n",
        "def evaluate_model(model, test_loader):\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    model.to(device) # Move model to the device\n",
        "\n",
        "    all_predicted = [] # List to store all predicted labels\n",
        "    all_labels = []    # List to store all actual labels\n",
        "\n",
        "    running_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # Use Cross-Entropy Loss for evaluation\n",
        "\n",
        "    # Disable gradient calculation during evaluation\n",
        "    with torch.no_grad():\n",
        "        for images, labels in test_loader:\n",
        "            # Move data to the device\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Update test loss\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "\n",
        "            # Calculate test accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # Store predicted and actual labels\n",
        "            all_predicted.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    # Calculate average test loss and accuracy\n",
        "    test_loss = running_loss / len(test_loader.dataset)\n",
        "    test_accuracy = correct / total\n",
        "\n",
        "    return test_loss, test_accuracy, all_predicted, all_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yb0EXXeqXlh4"
      },
      "outputs": [],
      "source": [
        "# Function to plot training and validation results (accuracy and loss)\n",
        "def plot_results(history, title=\"\"):\n",
        "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "    # Plot accuracy\n",
        "    ax1.plot(history['train_accuracy'], label='Training accuracy')\n",
        "    ax1.plot(history['val_accuracy'], label='Validation accuracy')\n",
        "    ax1.set_title(f'Model accuracy {title}')\n",
        "    ax1.set_xlabel('Epoch')\n",
        "    ax1.set_ylabel('Accuracy')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True)\n",
        "\n",
        "    # Plot loss\n",
        "    ax2.plot(history['train_loss'], label='Training loss')\n",
        "    ax2.plot(history['val_loss'], label='Validation loss')\n",
        "    ax2.set_title(f'Model loss {title}')\n",
        "    ax2.set_xlabel('Epoch')\n",
        "    ax2.set_ylabel('Loss')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJrON_kbXmDj"
      },
      "outputs": [],
      "source": [
        "# Function to plot the confusion matrix and print classification report\n",
        "def plot_confusion_matrix(y_true, y_pred, title=\"\"):\n",
        "    # Calculate the confusion matrix\n",
        "    cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    # Plot the confusion matrix using seaborn heatmap\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues') # annot=True shows values, fmt='d' formats as integers\n",
        "    plt.title(f'Confusion matrix {title}')\n",
        "    plt.xlabel('Predicted class')\n",
        "    plt.ylabel('Actual class')\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Print the classification report\n",
        "    print(classification_report(y_true, y_pred, target_names=['Muffin', 'Chihuahua']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JcAt0ceJXn3a"
      },
      "outputs": [],
      "source": [
        "# Function to save a sample of test predictions to a CSV file\n",
        "def save_sample_predictions(X_test, y_test, y_pred, save_path):\n",
        "    # Select 30 samples randomly, ensuring representation from both classes\n",
        "    true_labels = np.array(y_test)\n",
        "    predicted_labels = np.array(y_pred)\n",
        "\n",
        "    # Find indices for each class in the test set\n",
        "    muffin_indices = np.where(true_labels == 0)[0]\n",
        "    chihuahua_indices = np.where(true_labels == 1)[0]\n",
        "\n",
        "    # Select a specified number of samples from each class\n",
        "    num_muffin = min(15, len(muffin_indices))\n",
        "    num_chihuahua = min(15, len(chihuahua_indices))\n",
        "\n",
        "    # Randomly select indices from each class\n",
        "    selected_muffin = np.random.choice(muffin_indices, num_muffin, replace=False)\n",
        "    selected_chihuahua = np.random.choice(chihuahua_indices, num_chihuahua, replace=False)\n",
        "\n",
        "    # Combine the selected indices\n",
        "    selected_indices = np.concatenate([selected_muffin, selected_chihuahua])\n",
        "\n",
        "    # Prepare data for CSV\n",
        "    csv_headers = [\"Index\", \"Image Path\", \"Actual Label\", \"Predicted Label\"]\n",
        "\n",
        "    csv_rows = [csv_headers]\n",
        "    for i, idx in enumerate(selected_indices):\n",
        "        # Convert numerical labels back to class names\n",
        "        actual = \"Muffin\" if true_labels[idx] == 0 else \"Chihuahua\"\n",
        "        predicted = \"Muffin\" if predicted_labels[idx] == 0 else \"Chihuahua\"\n",
        "        row = [i, X_test[idx], actual, predicted]\n",
        "        csv_rows.append(row)\n",
        "\n",
        "    # Save to CSV file\n",
        "    with open(save_path, mode='w', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        writer.writerows(csv_rows)\n",
        "\n",
        "    print(f\"Sample predictions saved to: {save_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjgNIdC1XqVS"
      },
      "outputs": [],
      "source": [
        "# Function to experiment with different hyperparameter values\n",
        "def experiment_hyperparameter(\n",
        "    train_loader, val_loader, param_name, param_values,\n",
        "    architecture=HYPERPARAMS['architecture'],\n",
        "    dropout_rate=HYPERPARAMS['dropout_rate'],\n",
        "    use_batch_norm=HYPERPARAMS['use_batch_norm'],\n",
        "    activation=HYPERPARAMS['activation'],\n",
        "    optimizer_name=HYPERPARAMS['optimizer_name'],\n",
        "    lr=HYPERPARAMS['learning_rate'],\n",
        "    epochs=HYPERPARAMS['epochs'],\n",
        "    batch_size=HYPERPARAMS['batch_size'],\n",
        "    kernel_size=HYPERPARAMS['kernel_size'],\n",
        "    num_filters=HYPERPARAMS['num_filters'],\n",
        "    pool_size=HYPERPARAMS['pool_size']\n",
        "):\n",
        "    # Ensure the parameter name is supported for experimentation\n",
        "    assert param_name in ['architecture', 'dropout_rate', 'use_batch_norm', 'activation', 'optimizer_name'], \"Unsupported parameter\"\n",
        "\n",
        "    results = {} # Dictionary to store results for each parameter value\n",
        "\n",
        "    for val in param_values:\n",
        "        print(f\"Training with {param_name}={val}...\")\n",
        "\n",
        "        # Create a new model instance with the current parameter value\n",
        "        model_kwargs = {\n",
        "            'architecture': architecture,\n",
        "            'dropout_rate': dropout_rate,\n",
        "            'use_batch_norm': use_batch_norm,\n",
        "            'activation': activation,\n",
        "            'kernel_size': kernel_size,\n",
        "            'num_filters': num_filters,\n",
        "            'pool_size': pool_size\n",
        "        }\n",
        "\n",
        "        # Update the specific parameter being experimented\n",
        "        if param_name != 'optimizer_name': # Optimizer is not a model parameter\n",
        "            model_kwargs[param_name] = val\n",
        "\n",
        "        model = CNN(**model_kwargs)\n",
        "\n",
        "        # Select the optimizer based on the parameter being experimented or the default\n",
        "        curr_optimizer_name = optimizer_name\n",
        "        if param_name == 'optimizer_name':\n",
        "            curr_optimizer_name = val\n",
        "\n",
        "        if curr_optimizer_name == 'adam':\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "        elif curr_optimizer_name == 'sgd':\n",
        "            optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "        elif curr_optimizer_name == 'rmsprop':\n",
        "            optimizer = optim.RMSprop(model.parameters(), lr=lr)\n",
        "        else:\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)  # Default to Adam\n",
        "\n",
        "        criterion = nn.CrossEntropyLoss() # Loss function\n",
        "\n",
        "        # Train the model with the current parameter value\n",
        "        _, history = train_model(model, train_loader, val_loader, optimizer, criterion, epochs=epochs)\n",
        "\n",
        "        # Store the training history and best validation metrics\n",
        "        results[val] = {\n",
        "            'history': history,\n",
        "            'val_accuracy': max(history['val_accuracy']), # Best validation accuracy achieved\n",
        "            'val_loss': min(history['val_loss']),       # Minimum validation loss achieved\n",
        "        }\n",
        "\n",
        "    # Plot the validation accuracy and loss for each parameter value\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for val in param_values:\n",
        "        plt.plot(results[val]['history']['val_accuracy'], label=f'{param_name}={val}')\n",
        "    plt.title(f'Validation accuracy comparison ({param_name})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Accuracy')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for val in param_values:\n",
        "        plt.plot(results[val]['history']['val_loss'], label=f'{param_name}={val}')\n",
        "    plt.title(f'Validation loss comparison ({param_name})')\n",
        "    plt.xlabel('Epoch')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Find and print the parameter value that resulted in the best validation accuracy\n",
        "    best_val = max(results, key=lambda x: results[x]['val_accuracy'])\n",
        "    print(f\"Best {param_name}: {best_val}\")\n",
        "    print(f\"Validation accuracy: {results[best_val]['val_accuracy']:.4f}\")\n",
        "    print(f\"Validation loss: {results[best_val]['val_loss']:.4f}\")\n",
        "\n",
        "    return results, best_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ecr2SubvXttH"
      },
      "outputs": [],
      "source": [
        "# Function to create and evaluate the final best model based on experiment results\n",
        "def create_best_model(train_loader, val_loader, test_loader, X_test, y_test,\n",
        "                      architecture, dropout_rate, use_batch_norm, activation, optimizer_name, lr=HYPERPARAMS['learning_rate']):\n",
        "    # Create the best model with the optimal hyperparameters found\n",
        "    best_model = CNN(\n",
        "        architecture=architecture,\n",
        "        dropout_rate=dropout_rate,\n",
        "        use_batch_norm=use_batch_norm,\n",
        "        activation=activation\n",
        "    )\n",
        "\n",
        "    # Select the best optimizer\n",
        "    if optimizer_name == 'adam':\n",
        "        optimizer = optim.Adam(best_model.parameters(), lr=lr)\n",
        "    elif optimizer_name == 'sgd':\n",
        "        optimizer = optim.SGD(best_model.parameters(), lr=lr, momentum=0.9)\n",
        "    elif optimizer_name == 'rmsprop':\n",
        "        optimizer = optim.RMSprop(best_model.parameters(), lr=lr)\n",
        "    else:\n",
        "        optimizer = optim.Adam(best_model.parameters(), lr=lr) # Default\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss() # Loss function\n",
        "\n",
        "    # Train the best model\n",
        "    best_model, history = train_model(best_model, train_loader, val_loader, optimizer, criterion)\n",
        "\n",
        "    # Plot the training and validation results for the best model\n",
        "    plot_results(history, f\"(Best model: {architecture}, {activation}, {optimizer_name})\")\n",
        "\n",
        "    # Evaluate the best model on the test set\n",
        "    test_loss, test_accuracy, y_pred, y_true = evaluate_model(best_model, test_loader)\n",
        "    print(f\"Testing accuracy: {test_accuracy:.4f}\")\n",
        "    print(f\"Testing loss: {test_loss:.4f}\")\n",
        "\n",
        "    # Plot the confusion matrix for the test set\n",
        "    plot_confusion_matrix(y_true, y_pred, \"Best model\")\n",
        "\n",
        "    # Save a sample of the test predictions\n",
        "    save_sample_predictions(X_test, y_test, y_pred, f\"{output_dir}/test_predictions.csv\")\n",
        "\n",
        "    return best_model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "RWnq2VT_Xytl",
        "outputId": "8e3a108f-7f25-4a3c-84eb-8cce11f57d75"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    train_loader, val_loader, test_loader, X_test, y_test = prepare_data(\n",
        "        data_dir=data_dir,\n",
        "        batch_size=HYPERPARAMS['batch_size'],\n",
        "        test_size=HYPERPARAMS['test_size'],\n",
        "        val_size=HYPERPARAMS['val_size']\n",
        "    )\n",
        "\n",
        "    print(f\"Number of training batches: {len(train_loader)}\")\n",
        "    print(f\"Number of validation batches: {len(val_loader)}\")\n",
        "    print(f\"Number of testing batches: {len(test_loader)}\")\n",
        "\n",
        "    # Experiment 1: Architecture comparison\n",
        "    print(\"\\nEXPERIMENT NR.1: ARCHITECTURE COMPARISON\")\n",
        "    arch_results, best_arch = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        param_name='architecture',\n",
        "        param_values=['simple', 'medium', 'complex']\n",
        "    )\n",
        "\n",
        "    # Experiment 2: Dropout layers comparison\n",
        "    print(\"\\nEXPERIMENT NR.2: DROPOUT LAYERS COMPARISON\")\n",
        "    dropout_results, best_dropout = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        param_name='dropout_rate',\n",
        "        param_values=[0.0, 0.2, 0.5, 0.7]\n",
        "    )\n",
        "\n",
        "    # Experiment 3: Batch normalization comparison\n",
        "    print(\"\\nEXPERIMENT NR.3: BATCH NORMALISATION COMPARISON\")\n",
        "    bn_results, best_bn = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        dropout_rate=best_dropout,\n",
        "        param_name='use_batch_norm',\n",
        "        param_values=[False, True]\n",
        "    )\n",
        "\n",
        "    # Experiment 4: Activation functions comparison\n",
        "    print(\"\\nEXPERIMENT NR.4: ACTIVATION FUNCTIONS COMPARISON\")\n",
        "    act_results, best_activation = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        dropout_rate=best_dropout,\n",
        "        use_batch_norm=best_bn,\n",
        "        param_name='activation',\n",
        "        param_values=['relu', 'tanh', 'elu', 'selu']\n",
        "    )\n",
        "\n",
        "    # Experiment 5: Optimizer comparison\n",
        "    print(\"\\nEXPERIMENT NR.5: OPTIMIZER COMPARISON\")\n",
        "    opt_results, best_optimizer = experiment_hyperparameter(\n",
        "        train_loader, val_loader,\n",
        "        architecture=best_arch,\n",
        "        dropout_rate=best_dropout,\n",
        "        use_batch_norm=best_bn,\n",
        "        activation=best_activation,\n",
        "        param_name='optimizer_name',\n",
        "        param_values=['adam', 'sgd', 'rmsprop']\n",
        "    )\n",
        "\n",
        "    # Create and evaluate best model\n",
        "    print(\"\\nBEST MODEL TRAINING AND TESTING\")\n",
        "    best_model, best_history = create_best_model(\n",
        "        train_loader, val_loader, test_loader, X_test, y_test,\n",
        "        best_arch, best_dropout, best_bn, best_activation, best_optimizer\n",
        "    )\n",
        "\n",
        "    # Print best hyperparameters\n",
        "    print(\"\\nBEST HYPERPARAMETERS:\")\n",
        "    print(f\"Architecture: {best_arch}\")\n",
        "    print(f\"Dropout value: {best_dropout}\")\n",
        "    print(f\"Batch normalization: {best_bn}\")\n",
        "    print(f\"Activation function: {best_activation}\")\n",
        "    print(f\"Optimizer: {best_optimizer}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
